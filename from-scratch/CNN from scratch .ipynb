{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a51520f",
   "metadata": {},
   "source": [
    "#### CNN \n",
    "A Convolutional Neural Network (ConvNet/CNN) is a deep learning algorithm that processes input images, assigns importance to various image elements, and learns to distinguish them. Unlike traditional methods, ConvNets require minimal preprocessing and can automatically learn important filters or features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfeab15",
   "metadata": {},
   "source": [
    "### How to do it \n",
    "Based on the following paper https://arxiv.org/pdf/1511.08458.pdf, we need to define three layers: Convolutional, Pooling and  FullyConnected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cc1a6",
   "metadata": {},
   "source": [
    "CNN's core functions can be summarized as follows:\n",
    "\n",
    "- The input layer stores image pixel values.\n",
    "\n",
    "- Convolutional layers calculate neuron outputs connected to local image regions using weighted sums and apply ReLu activation.\n",
    "\n",
    "- The pooling layer downsamples spatial dimensions, reducing parameters.\n",
    "\n",
    "- Fully-connected layers generate class scores for classification, often with ReLu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb572e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the basic function: softmax, cross entropy and RELU\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def cross_entropy(x):\n",
    "    return -np.log(x)\n",
    "\n",
    "\n",
    "def regularized_cross_entropy(layers, lam, x):\n",
    "    loss = cross_entropy(x)\n",
    "    for layer in layers:\n",
    "        loss += lam * (np.linalg.norm(layer.get_weights()) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def ReLU(x, alpha=0.001):\n",
    "    return x * alpha if x < 0 else x\n",
    "\n",
    "\n",
    "def ReLU_derivative(x, alpha=0.01):\n",
    "    return alpha if x < 0 else 1\n",
    "\n",
    "\n",
    "def lr_schedule(learning_rate, iteration):\n",
    "    if iteration == 0:\n",
    "        return learning_rate\n",
    "    if (iteration >= 0) and (iteration <= 10000):\n",
    "        return learning_rate\n",
    "    if iteration > 10000:\n",
    "        return learning_rate * 0.1\n",
    "    if iteration > 30000:\n",
    "        return learning_rate * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee311b",
   "metadata": {},
   "source": [
    "Convolution layer: \n",
    "- Best way to undestand convolution operator: https://computationalthinking.mit.edu/Fall20/\n",
    "- How it's used in CNN:  CNN is a feed-forward, in the forward path the input data undergoes convolution, activation, pooling, flattening, fully connected layer processing. In the backward path the gradients of the loss with respect to the kernel's parameters are calculated. These gradients guide the adjustments made to the kernel's values as part of the weight update process. The goal is to learn the kernal through backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becdca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional:                                        \n",
    "    def __init__(self, name, num_filters=16, stride=1, size=3, activation=None):\n",
    "        self.name = name\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) * 0.1\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "        self.activation = activation\n",
    "        self.last_input = None\n",
    "        self.leakyReLU = np.vectorize(leakyReLU)\n",
    "        self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # keep track of last input for later backward propagation\n",
    "        self.last_input = image                             \n",
    "\n",
    "        input_dimension = image.shape[1]                                                \n",
    "        output_dimension = int((input_dimension - self.size) / self.stride) + 1         \n",
    "        # matrix to hold the values of the convolution\n",
    "        out = np.zeros((self.filters.shape[0], output_dimension, output_dimension))     \n",
    "        \n",
    "        #Apply convolution\n",
    "        for f in range(self.filters.shape[0]):              \n",
    "            tmp_y = out_y = 0                               \n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    patch = image[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n",
    "                    out[f, out_y, out_x] += np.sum(self.filters[f] * patch)\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        \n",
    "                               \n",
    "        self.ReLU(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, din, learn_rate=0.005):\n",
    "        input_dimension = self.last_input.shape[1]          \n",
    "\n",
    "                          \n",
    "        self.ReLU_derivative(din)\n",
    "\n",
    "        dout = np.zeros(self.last_input.shape)             \n",
    "        dfilt = np.zeros(self.filters.shape)                \n",
    "\n",
    "        for f in range(self.filters.shape[0]):              \n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    patch = self.last_input[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n",
    "                    dfilt[f] += np.sum(din[f, out_y, out_x] * patch, axis=0)\n",
    "                    dout[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size] += din[f, out_y, out_x] * self.filters[f]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        self.filters -= learn_rate * dfilt                 \n",
    "        return dout                                        \n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.filters, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a617daf",
   "metadata": {},
   "source": [
    "Polling layer: \n",
    "- This layer enables an image to be sub-sampled when it is classified by a neural network. The aim of this layer is to reduce the size of the images without modifying the important features of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000c7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:                                             \n",
    "    def __init__(self, name, stride=2, size=2):\n",
    "        self.name = name\n",
    "        self.last_input = None\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.last_input = image                             \n",
    "        num_channels, h_prev, w_prev = image.shape\n",
    "        h = int((h_prev - self.size) / self.stride) + 1     \n",
    "        w = int((w_prev - self.size) / self.stride) + 1\n",
    "\n",
    "        downsampled = np.zeros((num_channels, h, w))        \n",
    "\n",
    "        for i in range(num_channels):                       \n",
    "            curr_y = out_y = 0                              \n",
    "            while curr_y + self.size <= h_prev:             \n",
    "                curr_x = out_x = 0\n",
    "                while curr_x + self.size <= w_prev:         \n",
    "                    patch = image[i, curr_y:curr_y + self.size, curr_x:curr_x + self.size]\n",
    "                    downsampled[i, out_y, out_x] = np.max(patch)       \n",
    "                    curr_x += self.stride                              \n",
    "                    out_x += 1\n",
    "                curr_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return downsampled\n",
    "\n",
    "    def backward(self, din, learning_rate):\n",
    "        num_channels, orig_dim, *_ = self.last_input.shape      \n",
    "                                                                \n",
    "\n",
    "        dout = np.zeros(self.last_input.shape)                  \n",
    "\n",
    "        for c in range(num_channels):\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.size <= orig_dim:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= orig_dim:\n",
    "                    patch = self.last_input[c, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]    # obtain index of largest\n",
    "                    (x, y) = np.unravel_index(np.nanargmax(patch), patch.shape)                     # value in patch\n",
    "                    dout[c, tmp_y + x, tmp_x + y] += din[c, out_y, out_x]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def get_weights(self):                          \n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e44d7",
   "metadata": {},
   "source": [
    "FullyConnected Layer : \n",
    "- The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of ANN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcdc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:                               \n",
    "    def __init__(self, name, nodes1, nodes2, activation):\n",
    "        self.name = name\n",
    "        self.weights = np.random.randn(nodes1, nodes2) * 0.1\n",
    "        self.biases = np.zeros(nodes2)\n",
    "        self.activation = activation\n",
    "        self.last_input_shape = None\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "        self.leakyReLU = np.vectorize(leakyReLU)\n",
    "        self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape         \n",
    "                                                \n",
    "\n",
    "        input = input.flatten()                                 \n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases      \n",
    "\n",
    "        if self.activation == 'relu':                          \n",
    "            self.leakyReLU(output)\n",
    "\n",
    "        self.last_input = input                     \n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, din, learning_rate=0.005):\n",
    "        if self.activation == 'relu':                             \n",
    "           self.leakyReLU_derivative(din)\n",
    "\n",
    "        self.last_input = np.expand_dims(self.last_input, axis=1)\n",
    "        din = np.expand_dims(din, axis=1)\n",
    "\n",
    "        dw = np.dot(self.last_input, np.transpose(din))           \n",
    "        db = np.sum(din, axis=1).reshape(self.biases.shape)       \n",
    "\n",
    "        self.weights -= learning_rate * dw                        \n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "        dout = np.dot(self.weights, din)\n",
    "        return dout.reshape(self.last_input_shape)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.weights, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b56b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe73991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
